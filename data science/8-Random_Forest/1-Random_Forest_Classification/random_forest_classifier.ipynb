{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"colab":{"name":"random_forest_classifier.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"n7Y2bidUzon5","colab_type":"text"},"source":["Random Forest is an ensemble type of model which consists of many other models like Decision Trees and etc."]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"CoXruXSPzon6","colab_type":"text"},"source":["Random Sampling of Observations(Bagging) and Random Subset of Features.(if you don't know these, read it up in Random Forest Regression notebook)"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"Xje9hW6Tzon7","colab_type":"code","colab":{},"outputId":"af53d22c-e321-492a-d273-daf6184ca7b9"},"source":["%%HTML\n","<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/loNcrMjYh64\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/loNcrMjYh64\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"_RMe3zwEzooB","colab_type":"text"},"source":["Random Forest Classifier in Python"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"q46SJuhuzooC","colab_type":"code","colab":{}},"source":["import pandas as pd"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"W50iIZ12zooG","colab_type":"code","colab":{}},"source":["transact_dataset = pd.read_csv('creditcard.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"sDTtYotWzooJ","colab_type":"text"},"source":["The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions.<br>\n","Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. "]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"3Xg-5QF3zooK","colab_type":"code","colab":{},"outputId":"8551ec0e-d386-4d8c-8c90-42a2b6d73de1"},"source":["transact_dataset.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style>\n","    .dataframe thead tr:only-child th {\n","        text-align: right;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: left;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Time</th>\n","      <th>V1</th>\n","      <th>V2</th>\n","      <th>V3</th>\n","      <th>V4</th>\n","      <th>V5</th>\n","      <th>V6</th>\n","      <th>V7</th>\n","      <th>V8</th>\n","      <th>V9</th>\n","      <th>...</th>\n","      <th>V21</th>\n","      <th>V22</th>\n","      <th>V23</th>\n","      <th>V24</th>\n","      <th>V25</th>\n","      <th>V26</th>\n","      <th>V27</th>\n","      <th>V28</th>\n","      <th>Amount</th>\n","      <th>Class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>-1.359807</td>\n","      <td>-0.072781</td>\n","      <td>2.536347</td>\n","      <td>1.378155</td>\n","      <td>-0.338321</td>\n","      <td>0.462388</td>\n","      <td>0.239599</td>\n","      <td>0.098698</td>\n","      <td>0.363787</td>\n","      <td>...</td>\n","      <td>-0.018307</td>\n","      <td>0.277838</td>\n","      <td>-0.110474</td>\n","      <td>0.066928</td>\n","      <td>0.128539</td>\n","      <td>-0.189115</td>\n","      <td>0.133558</td>\n","      <td>-0.021053</td>\n","      <td>149.62</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>1.191857</td>\n","      <td>0.266151</td>\n","      <td>0.166480</td>\n","      <td>0.448154</td>\n","      <td>0.060018</td>\n","      <td>-0.082361</td>\n","      <td>-0.078803</td>\n","      <td>0.085102</td>\n","      <td>-0.255425</td>\n","      <td>...</td>\n","      <td>-0.225775</td>\n","      <td>-0.638672</td>\n","      <td>0.101288</td>\n","      <td>-0.339846</td>\n","      <td>0.167170</td>\n","      <td>0.125895</td>\n","      <td>-0.008983</td>\n","      <td>0.014724</td>\n","      <td>2.69</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1.0</td>\n","      <td>-1.358354</td>\n","      <td>-1.340163</td>\n","      <td>1.773209</td>\n","      <td>0.379780</td>\n","      <td>-0.503198</td>\n","      <td>1.800499</td>\n","      <td>0.791461</td>\n","      <td>0.247676</td>\n","      <td>-1.514654</td>\n","      <td>...</td>\n","      <td>0.247998</td>\n","      <td>0.771679</td>\n","      <td>0.909412</td>\n","      <td>-0.689281</td>\n","      <td>-0.327642</td>\n","      <td>-0.139097</td>\n","      <td>-0.055353</td>\n","      <td>-0.059752</td>\n","      <td>378.66</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.0</td>\n","      <td>-0.966272</td>\n","      <td>-0.185226</td>\n","      <td>1.792993</td>\n","      <td>-0.863291</td>\n","      <td>-0.010309</td>\n","      <td>1.247203</td>\n","      <td>0.237609</td>\n","      <td>0.377436</td>\n","      <td>-1.387024</td>\n","      <td>...</td>\n","      <td>-0.108300</td>\n","      <td>0.005274</td>\n","      <td>-0.190321</td>\n","      <td>-1.175575</td>\n","      <td>0.647376</td>\n","      <td>-0.221929</td>\n","      <td>0.062723</td>\n","      <td>0.061458</td>\n","      <td>123.50</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2.0</td>\n","      <td>-1.158233</td>\n","      <td>0.877737</td>\n","      <td>1.548718</td>\n","      <td>0.403034</td>\n","      <td>-0.407193</td>\n","      <td>0.095921</td>\n","      <td>0.592941</td>\n","      <td>-0.270533</td>\n","      <td>0.817739</td>\n","      <td>...</td>\n","      <td>-0.009431</td>\n","      <td>0.798278</td>\n","      <td>-0.137458</td>\n","      <td>0.141267</td>\n","      <td>-0.206010</td>\n","      <td>0.502292</td>\n","      <td>0.219422</td>\n","      <td>0.215153</td>\n","      <td>69.99</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows Ã— 31 columns</p>\n","</div>"],"text/plain":["   Time        V1        V2        V3        V4        V5        V6        V7  \\\n","0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n","1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n","2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n","3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n","4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n","\n","         V8        V9  ...         V21       V22       V23       V24  \\\n","0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n","1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n","2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n","3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n","4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n","\n","        V25       V26       V27       V28  Amount  Class  \n","0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n","1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n","2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n","3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n","4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n","\n","[5 rows x 31 columns]"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"hGIQcvAxzooN","colab_type":"code","colab":{},"outputId":"475775a9-d189-46a3-abd5-cd626309de57"},"source":["transact_dataset.isnull().any()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Time      False\n","V1        False\n","V2        False\n","V3        False\n","V4        False\n","V5        False\n","V6        False\n","V7        False\n","V8        False\n","V9        False\n","V10       False\n","V11       False\n","V12       False\n","V13       False\n","V14       False\n","V15       False\n","V16       False\n","V17       False\n","V18       False\n","V19       False\n","V20       False\n","V21       False\n","V22       False\n","V23       False\n","V24       False\n","V25       False\n","V26       False\n","V27       False\n","V28       False\n","Amount    False\n","Class     False\n","dtype: bool"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"jR8pEXX_zooQ","colab_type":"text"},"source":["As we can see, none of the columns have empty values. Apparently, the data was normlizied in order to use for machine learning models"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"8XL0MZK2zooR","colab_type":"text"},"source":["Splitting dataset into features and labels. features x=v1 to v28 and amount and label y= class."]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"KBQ59fcuzooS","colab_type":"code","colab":{},"outputId":"fc07e815-f3fa-424c-f108-1291a8d28b4d"},"source":["X, y = transact_dataset.drop(\"Class\", axis=1), transact_dataset[\"Class\"]\n","print(X)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["            Time         V1         V2        V3        V4        V5  \\\n","0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n","1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n","2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n","3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n","4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n","5            2.0  -0.425966   0.960523  1.141109 -0.168252  0.420987   \n","6            4.0   1.229658   0.141004  0.045371  1.202613  0.191881   \n","7            7.0  -0.644269   1.417964  1.074380 -0.492199  0.948934   \n","8            7.0  -0.894286   0.286157 -0.113192 -0.271526  2.669599   \n","9            9.0  -0.338262   1.119593  1.044367 -0.222187  0.499361   \n","10          10.0   1.449044  -1.176339  0.913860 -1.375667 -1.971383   \n","11          10.0   0.384978   0.616109 -0.874300 -0.094019  2.924584   \n","12          10.0   1.249999  -1.221637  0.383930 -1.234899 -1.485419   \n","13          11.0   1.069374   0.287722  0.828613  2.712520 -0.178398   \n","14          12.0  -2.791855  -0.327771  1.641750  1.767473 -0.136588   \n","15          12.0  -0.752417   0.345485  2.057323 -1.468643 -1.158394   \n","16          12.0   1.103215  -0.040296  1.267332  1.289091 -0.735997   \n","17          13.0  -0.436905   0.918966  0.924591 -0.727219  0.915679   \n","18          14.0  -5.401258  -5.450148  1.186305  1.736239  3.049106   \n","19          15.0   1.492936  -1.029346  0.454795 -1.438026 -1.555434   \n","20          16.0   0.694885  -1.361819  1.029221  0.834159 -1.191209   \n","21          17.0   0.962496   0.328461 -0.171479  2.109204  1.129566   \n","22          18.0   1.166616   0.502120 -0.067300  2.261569  0.428804   \n","23          18.0   0.247491   0.277666  1.185471 -0.092603 -1.314394   \n","24          22.0  -1.946525  -0.044901 -0.405570 -1.013057  2.941968   \n","25          22.0  -2.074295  -0.121482  1.322021  0.410008  0.295198   \n","26          23.0   1.173285   0.353498  0.283905  1.133563 -0.172577   \n","27          23.0   1.322707  -0.174041  0.434555  0.576038 -0.836758   \n","28          23.0  -0.414289   0.905437  1.727453  1.473471  0.007443   \n","29          23.0   1.059387  -0.175319  1.266130  1.186110 -0.786002   \n","...          ...        ...        ...       ...       ...       ...   \n","284777  172764.0   2.079137  -0.028723 -1.343392  0.358000 -0.045791   \n","284778  172764.0  -0.764523   0.588379 -0.907599 -0.418847  0.901528   \n","284779  172766.0   1.975178  -0.616244 -2.628295 -0.406246  2.327804   \n","284780  172766.0  -1.727503   1.108356  2.219561  1.148583 -0.884199   \n","284781  172766.0  -1.139015  -0.155510  1.894478 -1.138957  1.451777   \n","284782  172767.0  -0.268061   2.540315 -1.400915  4.846661  0.639105   \n","284783  172768.0  -1.796092   1.929178 -2.828417 -1.689844  2.199572   \n","284784  172768.0  -0.669662   0.923769 -1.543167 -1.560729  2.833960   \n","284785  172768.0   0.032887   0.545338 -1.185844 -1.729828  2.932315   \n","284786  172768.0  -2.076175   2.142238 -2.522704 -1.888063  1.982785   \n","284787  172769.0  -1.029719  -1.110670 -0.636179 -0.840816  2.424360   \n","284788  172770.0   2.007418  -0.280235 -0.208113  0.335261 -0.715798   \n","284789  172770.0  -0.446951   1.302212 -0.168583  0.981577  0.578957   \n","284790  172771.0  -0.515513   0.971950 -1.014580 -0.677037  0.912430   \n","284791  172774.0  -0.863506   0.874701  0.420358 -0.530365  0.356561   \n","284792  172774.0  -0.724123   1.485216 -1.132218 -0.607190  0.709499   \n","284793  172775.0   1.971002  -0.699067 -1.697541 -0.617643  1.718797   \n","284794  172777.0  -1.266580  -0.400461  0.956221 -0.723919  1.531993   \n","284795  172778.0 -12.516732  10.187818 -8.476671 -2.510473 -4.586669   \n","284796  172780.0   1.884849  -0.143540 -0.999943  1.506772 -0.035300   \n","284797  172782.0  -0.241923   0.712247  0.399806 -0.463406  0.244531   \n","284798  172782.0   0.219529   0.881246 -0.635891  0.960928 -0.152971   \n","284799  172783.0  -1.775135  -0.004235  1.189786  0.331096  1.196063   \n","284800  172784.0   2.039560  -0.175233 -1.196825  0.234580 -0.008713   \n","284801  172785.0   0.120316   0.931005 -0.546012 -0.745097  1.130314   \n","284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n","284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n","284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n","284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n","284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n","\n","              V6        V7        V8        V9   ...         V20       V21  \\\n","0       0.462388  0.239599  0.098698  0.363787   ...    0.251412 -0.018307   \n","1      -0.082361 -0.078803  0.085102 -0.255425   ...   -0.069083 -0.225775   \n","2       1.800499  0.791461  0.247676 -1.514654   ...    0.524980  0.247998   \n","3       1.247203  0.237609  0.377436 -1.387024   ...   -0.208038 -0.108300   \n","4       0.095921  0.592941 -0.270533  0.817739   ...    0.408542 -0.009431   \n","5      -0.029728  0.476201  0.260314 -0.568671   ...    0.084968 -0.208254   \n","6       0.272708 -0.005159  0.081213  0.464960   ...   -0.219633 -0.167716   \n","7       0.428118  1.120631 -3.807864  0.615375   ...   -0.156742  1.943465   \n","8       3.721818  0.370145  0.851084 -0.392048   ...    0.052736 -0.073425   \n","9      -0.246761  0.651583  0.069539 -0.736727   ...    0.203711 -0.246914   \n","10     -0.629152 -1.423236  0.048456 -1.720408   ...   -0.387226 -0.009302   \n","11      3.317027  0.470455  0.538247 -0.558895   ...    0.125992  0.049924   \n","12     -0.753230 -0.689405 -0.227487 -2.094011   ...   -0.102756 -0.231809   \n","13      0.337544 -0.096717  0.115982 -0.221083   ...   -0.153197 -0.036876   \n","14      0.807596 -0.422911 -1.907107  0.755713   ...   -1.582122  1.151663   \n","15     -0.077850 -0.608581  0.003603 -0.436167   ...    0.263451  0.499625   \n","16      0.288069 -0.586057  0.189380  0.782333   ...   -0.113910 -0.024612   \n","17     -0.127867  0.707642  0.087962 -0.665271   ...   -0.047021 -0.194796   \n","18     -1.763406 -1.559738  0.160842  1.233090   ...   -2.196848 -0.503600   \n","19     -0.720961 -1.080664 -0.053127 -1.978682   ...   -0.387910 -0.177650   \n","20      1.309109 -0.878586  0.445290 -0.446196   ...   -0.138334 -0.295583   \n","21      1.696038  0.107712  0.521502 -1.191311   ...   -0.269321  0.143997   \n","22      0.089474  0.241147  0.138082 -0.989162   ...   -0.307169  0.018702   \n","23     -0.150116 -0.946365 -1.617935  1.544071   ...   -0.230983  1.650180   \n","24      2.955053 -0.063063  0.855546  0.049967   ...   -0.216715 -0.579526   \n","25     -0.959537  0.543985 -0.104627  0.475664   ...   -0.386694 -0.403639   \n","26     -0.916054  0.369025 -0.327260 -0.246651   ...    0.027878  0.067003   \n","27     -0.831083 -0.264905 -0.220982 -1.071425   ...   -0.522951 -0.284376   \n","28     -0.200331  0.740228 -0.029247 -0.593392   ...    0.097308  0.077237   \n","29      0.578435 -0.767084  0.401046  0.699500   ...   -0.178023  0.013676   \n","...          ...       ...       ...       ...   ...         ...       ...   \n","284777 -1.345452  0.227476 -0.378355  0.665911   ...   -0.272447  0.235758   \n","284778 -0.760802  0.758545  0.414698 -0.730854   ...    0.024870  0.003530   \n","284779  3.664740 -0.533297  0.842937  1.128798   ...   -0.168378  0.086043   \n","284780  0.793083 -0.527298  0.866429  0.853819   ...    0.331940 -0.094708   \n","284781  0.093598  0.191353  0.092211 -0.062621   ...    0.341409 -0.191027   \n","284782  0.186479 -0.045911  0.936448 -2.419986   ...    0.111808 -0.263889   \n","284783  3.123732 -0.270714  1.657495  0.465804   ...    0.319366  0.271170   \n","284784  3.240843  0.181576  1.282746 -0.893890   ...    0.000965  0.183856   \n","284785  3.401529  0.337434  0.925377 -0.165663   ...    0.022677 -0.266113   \n","284786  3.732950 -1.217430 -0.536644  0.272867   ...   -0.308523  2.016666   \n","284787 -2.956733  0.283610 -0.332656 -0.247488   ...    0.218776  0.353722   \n","284788 -0.751373 -0.458972 -0.140140  0.959971   ...   -0.143294 -0.208260   \n","284789 -0.605641  1.253430 -1.042610 -0.417116   ...   -0.203306  0.851800   \n","284790 -0.316187  0.396137  0.532364 -0.224606   ...   -0.177211 -0.280302   \n","284791 -1.046238  0.757051  0.230473 -0.506856   ...   -0.162132 -0.108846   \n","284792 -0.482638  0.548393  0.343003 -0.226323   ...   -0.077202  0.414621   \n","284793  3.911336 -1.259306  1.056209  1.315006   ...   -0.153581  0.188758   \n","284794 -1.788600  0.314741  0.004704  0.013857   ...   -0.029539 -0.157831   \n","284795 -1.394465 -3.632516  5.498583  4.893089   ...    3.490065 -0.944759   \n","284796 -0.613638  0.190241 -0.249058  0.666458   ...   -0.153997  0.144008   \n","284797 -1.343668  0.929369 -0.206210  0.106234   ...   -0.139512 -0.228876   \n","284798 -1.014307  0.427126  0.121340 -0.285670   ...    0.006666  0.099936   \n","284799  5.519980 -1.518185  2.080825  1.159498   ...    0.348176  0.103302   \n","284800 -0.726571  0.017050 -0.118228  0.435402   ...   -0.256922 -0.268048   \n","284801 -0.235973  0.812722  0.115093 -0.204064   ...    0.000676 -0.314205   \n","284802 -2.606837 -4.918215  7.305334  1.914428   ...    1.475829  0.213454   \n","284803  1.058415  0.024330  0.294869  0.584800   ...    0.059616  0.214205   \n","284804  3.031260 -0.296827  0.708417  0.432454   ...    0.001396  0.232045   \n","284805  0.623708 -0.686180  0.679145  0.392087   ...    0.127434  0.265245   \n","284806 -0.649617  1.577006 -0.414650  0.486180   ...    0.382948  0.261057   \n","\n","             V22       V23       V24       V25       V26       V27       V28  \\\n","0       0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n","1      -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n","2       0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n","3       0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n","4       0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n","5      -0.559825 -0.026398 -0.371427 -0.232794  0.105915  0.253844  0.081080   \n","6      -0.270710 -0.154104 -0.780055  0.750137 -0.257237  0.034507  0.005168   \n","7      -1.015455  0.057504 -0.649709 -0.415267 -0.051634 -1.206921 -1.085339   \n","8      -0.268092 -0.204233  1.011592  0.373205 -0.384157  0.011747  0.142404   \n","9      -0.633753 -0.120794 -0.385050 -0.069733  0.094199  0.246219  0.083076   \n","10      0.313894  0.027740  0.500512  0.251367 -0.129478  0.042850  0.016253   \n","11      0.238422  0.009130  0.996710 -0.767315 -0.492208  0.042472 -0.054337   \n","12     -0.483285  0.084668  0.392831  0.161135 -0.354990  0.026416  0.042422   \n","13      0.074412 -0.071407  0.104744  0.548265  0.104094  0.021491  0.021293   \n","14      0.222182  1.020586  0.028317 -0.232746 -0.235557 -0.164778 -0.030154   \n","15      1.353650 -0.256573 -0.065084 -0.039124 -0.087086 -0.180998  0.129394   \n","16      0.196002  0.013802  0.103758  0.364298 -0.382261  0.092809  0.037051   \n","17     -0.672638 -0.156858 -0.888386 -0.342413 -0.049027  0.079692  0.131024   \n","18      0.984460  2.458589  0.042119 -0.481631 -0.621272  0.392053  0.949594   \n","19     -0.175074  0.040002  0.295814  0.332931 -0.220385  0.022298  0.007602   \n","20     -0.571955 -0.050881 -0.304215  0.072001 -0.422234  0.086553  0.063499   \n","21      0.402492 -0.048508 -1.371866  0.390814  0.199964  0.016371 -0.014605   \n","22     -0.061972 -0.103855 -0.370415  0.603200  0.108556 -0.040521 -0.011418   \n","23      0.200454 -0.185353  0.423073  0.820591 -0.227632  0.336634  0.250475   \n","24     -0.799229  0.870300  0.983421  0.321201  0.149650  0.707519  0.014600   \n","25     -0.227404  0.742435  0.398535  0.249212  0.274404  0.359969  0.243232   \n","26      0.227812 -0.150487  0.435045  0.724825 -0.337082  0.016368  0.030041   \n","27     -0.323357 -0.037710  0.347151  0.559639 -0.280158  0.042335  0.028822   \n","28      0.457331 -0.038500  0.642522 -0.183891 -0.277464  0.182687  0.152665   \n","29      0.213734  0.014462  0.002951  0.294638 -0.395070  0.081461  0.024220   \n","...          ...       ...       ...       ...       ...       ...       ...   \n","284777  0.829758 -0.002063  0.001344  0.262183 -0.105327 -0.022363 -0.060283   \n","284778 -0.431876  0.141759  0.587119 -0.200998  0.267337 -0.152951 -0.065285   \n","284779  0.543613 -0.032129  0.768379  0.477688 -0.031833  0.014151 -0.066542   \n","284780  0.236818 -0.204280  1.158185  0.627801 -0.399981  0.510818  0.233265   \n","284781 -0.631658 -0.147249  0.212931  0.354257 -0.241068 -0.161717 -0.149188   \n","284782 -0.857904  0.235172 -0.681794 -0.668894  0.044657 -0.066751 -0.072447   \n","284783  1.145750  0.084783  0.721269 -0.529906 -0.240117  0.129126 -0.080620   \n","284784  0.202670 -0.373023  0.651122  1.073823  0.844590 -0.286676 -0.187719   \n","284785 -0.716336  0.108519  0.688519 -0.460220  0.161939  0.265368  0.090245   \n","284786 -1.588269  0.588482  0.632444 -0.201064  0.199251  0.438657  0.172923   \n","284787  0.488487  0.293632  0.107812 -0.935586  1.138216  0.025271  0.255347   \n","284788 -0.430347  0.416765  0.064819 -0.608337  0.268436 -0.028069 -0.041367   \n","284789  0.305268 -0.148093 -0.038712  0.010209 -0.362666  0.503092  0.229921   \n","284790 -0.849919  0.300245  0.000607 -0.376379  0.128660 -0.015205 -0.021486   \n","284791 -0.480820 -0.074513 -0.003988 -0.113149  0.280378 -0.077310  0.023079   \n","284792  1.307511 -0.059545  0.242669 -0.665424 -0.269869 -0.170579 -0.030692   \n","284793  0.694418  0.163002  0.726365 -0.058282 -0.191813  0.061858 -0.043716   \n","284794 -0.883365  0.088485 -0.076790 -0.095833  0.132720 -0.028468  0.126494   \n","284795 -1.565026  0.890675 -1.253276  1.786717  0.320763  2.090712  1.232864   \n","284796  0.634646 -0.042114 -0.053206  0.316403 -0.461441  0.018265 -0.041068   \n","284797 -0.514376  0.279598  0.371441 -0.559238  0.113144  0.131507  0.081265   \n","284798  0.337120  0.251791  0.057688 -1.508368  0.144023  0.181205  0.215243   \n","284799  0.654850 -0.348929  0.745323  0.704545 -0.127579  0.454379  0.130308   \n","284800 -0.717211  0.297930 -0.359769 -0.315610  0.201114 -0.080826 -0.075071   \n","284801 -0.808520  0.050343  0.102800 -0.435870  0.124079  0.217940  0.068803   \n","284802  0.111864  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731   \n","284803  0.924384  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   \n","284804  0.578229 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   \n","284805  0.800049 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   \n","284806  0.643078  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649   \n","\n","        Amount  \n","0       149.62  \n","1         2.69  \n","2       378.66  \n","3       123.50  \n","4        69.99  \n","5         3.67  \n","6         4.99  \n","7        40.80  \n","8        93.20  \n","9         3.68  \n","10        7.80  \n","11        9.99  \n","12      121.50  \n","13       27.50  \n","14       58.80  \n","15       15.99  \n","16       12.99  \n","17        0.89  \n","18       46.80  \n","19        5.00  \n","20      231.71  \n","21       34.09  \n","22        2.28  \n","23       22.75  \n","24        0.89  \n","25       26.43  \n","26       41.88  \n","27       16.00  \n","28       33.00  \n","29       12.99  \n","...        ...  \n","284777    1.00  \n","284778   80.00  \n","284779   25.00  \n","284780   30.00  \n","284781   13.00  \n","284782   12.82  \n","284783   11.46  \n","284784   40.00  \n","284785    1.79  \n","284786    8.95  \n","284787    9.99  \n","284788    3.99  \n","284789   60.50  \n","284790    9.81  \n","284791   20.32  \n","284792    3.99  \n","284793    4.99  \n","284794    0.89  \n","284795    9.87  \n","284796   60.00  \n","284797    5.49  \n","284798   24.05  \n","284799   79.99  \n","284800    2.68  \n","284801    2.69  \n","284802    0.77  \n","284803   24.79  \n","284804   67.88  \n","284805   10.00  \n","284806  217.00  \n","\n","[284807 rows x 30 columns]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"l5TAmov6zooW","colab_type":"code","colab":{},"outputId":"904daf8a-cdb1-4238-819a-b36d0e68c3a9"},"source":["#it shows 492 transactions are wrong\n","print(y)\n","print(sum(y))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0         0\n","1         0\n","2         0\n","3         0\n","4         0\n","5         0\n","6         0\n","7         0\n","8         0\n","9         0\n","10        0\n","11        0\n","12        0\n","13        0\n","14        0\n","15        0\n","16        0\n","17        0\n","18        0\n","19        0\n","20        0\n","21        0\n","22        0\n","23        0\n","24        0\n","25        0\n","26        0\n","27        0\n","28        0\n","29        0\n","         ..\n","284777    0\n","284778    0\n","284779    0\n","284780    0\n","284781    0\n","284782    0\n","284783    0\n","284784    0\n","284785    0\n","284786    0\n","284787    0\n","284788    0\n","284789    0\n","284790    0\n","284791    0\n","284792    0\n","284793    0\n","284794    0\n","284795    0\n","284796    0\n","284797    0\n","284798    0\n","284799    0\n","284800    0\n","284801    0\n","284802    0\n","284803    0\n","284804    0\n","284805    0\n","284806    0\n","Name: Class, Length: 284807, dtype: int64\n","492\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"7f8drDxvzooZ","colab_type":"text"},"source":["Splitting features and labels into Train and Test split "]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"UM66qwsJzooa","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"ljgKGVzlzood","colab_type":"text"},"source":["Creating the Random Forest Classifier model and fitting the Training set"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"ZNfJcdDDzooe","colab_type":"code","colab":{},"outputId":"23033478-a49e-4326-a414-759a6bbd2dfb"},"source":["from sklearn.ensemble import RandomForestClassifier\n","rf_clas_1 = RandomForestClassifier(random_state=0)\n","rf_clas_1.fit(X_train, y_train)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n","            max_depth=None, max_features='auto', max_leaf_nodes=None,\n","            min_impurity_decrease=0.0, min_impurity_split=None,\n","            min_samples_leaf=1, min_samples_split=2,\n","            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n","            oob_score=False, random_state=0, verbose=0, warm_start=False)"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"Ko9jIToTzooh","colab_type":"text"},"source":["Predicting Test set "]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"7ZKsPS33zooi","colab_type":"code","colab":{}},"source":["y_pred = rf_clas_1.predict(X_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"cRjjh4unzool","colab_type":"code","colab":{}},"source":["from sklearn.metrics import confusion_matrix"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"A9o3haMAzoop","colab_type":"text"},"source":["Creating confusion matrix to evaluate the model's performance"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"aqggzREqzooq","colab_type":"code","colab":{},"outputId":"b34d8e91-8488-4e1a-b777-6f5c684fb400"},"source":["cm = confusion_matrix(y_test, y_pred)\n","cm"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[93817,     8],\n","       [   41,   121]], dtype=int64)"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"O9-Vj11Nzoot","colab_type":"text"},"source":["Interpreting the Confusion Matrix:\n","<br>Model predicted 93817 as non-fraudulent correctly.\n","<br>Model predicted 121 as fraudulent correctly.\n","<br>Model made a mistake predicting 8 transactions as fraudulent, but they were not.\n","<br>Model made a mistake predicting 41 transactions as non-fraudulent, but they were."]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"0tSP-KWizoou","colab_type":"code","colab":{},"outputId":"6bc1c9ca-baef-4b50-9bd6-86472cf86581"},"source":["len(y_test[y_test==1])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["162"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"dmRMQAv3zooy","colab_type":"text"},"source":["The dataset was quite imbalanced because of the proportion of fraudulent and non-fraudulent transactions. There were only 492 out of 284,807 transactions."]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"Gs-jSfJLzooz","colab_type":"text"},"source":["That's why I will evaluate the model based on only Fraudulent transactions"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"kJ_pToTlzoo0","colab_type":"text"},"source":["There were only 145 fraudulent transactions in testing set and 112 of them were predicted correctly "]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"3Fl2Y48czoo1","colab_type":"code","colab":{},"outputId":"38561a2b-2377-402e-dd50-e6c58fec3449"},"source":["eval_1 = 121/162\n","eval_1\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7469135802469136"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"WTrcCifGzoo4","colab_type":"text"},"source":["Now, let's tune the parameters of the model in order to get better prediction "]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"VnLLaWIjzoo5","colab_type":"text"},"source":["Getting important features"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"scrolled":true,"id":"O2XFPliOzoo7","colab_type":"code","colab":{},"outputId":"d29933ed-d931-412d-9e16-56fb55ca274e"},"source":["feature_imp = pd.DataFrame(rf_clas_1.feature_importances_, index=X_test.columns, columns=[\"Importance\"])\n","print(feature_imp.sort_values(['Importance']))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["        Importance\n","V23       0.006436\n","V27       0.006828\n","V13       0.007511\n","V25       0.008177\n","V22       0.009194\n","Amount    0.010028\n","V24       0.010364\n","V5        0.010528\n","V28       0.010655\n","V8        0.010898\n","V18       0.011135\n","V20       0.011276\n","V19       0.013398\n","V1        0.014546\n","V15       0.015744\n","V21       0.016539\n","V4        0.017394\n","V3        0.019027\n","V26       0.020087\n","Time      0.020209\n","V6        0.021849\n","V2        0.022113\n","V7        0.026987\n","V9        0.033672\n","V16       0.043477\n","V10       0.079687\n","V11       0.093105\n","V12       0.099886\n","V14       0.137389\n","V17       0.191859\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"6WgSky-Rzoo_","colab_type":"text"},"source":["Creating another Random Forest Classifier model"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"8r_II1ONzopC","colab_type":"code","colab":{}},"source":["rf_clas_2 = RandomForestClassifier(n_estimators=100, criterion=\"entropy\", random_state=0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"TyiJajibzopH","colab_type":"code","colab":{},"outputId":"36e44269-a291-428f-edcc-c54d5cc687d6"},"source":["rf_clas_2.fit(X_train, y_train)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n","                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n","                       min_impurity_decrease=0.0, min_impurity_split=None,\n","                       min_samples_leaf=1, min_samples_split=2,\n","                       min_weight_fraction_leaf=0.0, n_estimators=100,\n","                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n","                       warm_start=False)"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"dAzpaTw3zopL","colab_type":"text"},"source":["Predicting test set"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"nphruEygzopN","colab_type":"code","colab":{}},"source":["y_pred_2 = rf_clas_2.predict(X_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"bPj1Wdn0zopQ","colab_type":"text"},"source":["Evaluating model's performance"]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"4cse22NbzopQ","colab_type":"code","colab":{},"outputId":"fb9e325a-e72b-4c3c-cd1b-7fbbf1d04d8d"},"source":["cm_2 = confusion_matrix(y_test, y_pred_2)\n","cm_2"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[93818,     7],\n","       [   37,   125]], dtype=int64)"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"pfiRNJEQzopV","colab_type":"code","colab":{}},"source":["rf_clas_3 = RandomForestClassifier(n_estimators=100, random_state=0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"1pVqx9I_zopZ","colab_type":"code","colab":{},"outputId":"b426cf98-b130-4510-e4a4-e7dbe54097d9"},"source":["rf_clas_3.fit(X_train, y_train)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n","                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n","                       min_impurity_decrease=0.0, min_impurity_split=None,\n","                       min_samples_leaf=1, min_samples_split=2,\n","                       min_weight_fraction_leaf=0.0, n_estimators=100,\n","                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n","                       warm_start=False)"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"ufF9FrOQzopc","colab_type":"code","colab":{}},"source":["y_pred_3 = rf_clas_2.predict(X_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"9bEdGrZfzopg","colab_type":"code","colab":{},"outputId":"3cf9ae7d-6273-4f73-a950-d0374d7c83f6"},"source":["cm_3 = confusion_matrix(y_test, y_pred_3)\n","cm_3"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[93818,     7],\n","       [   37,   125]], dtype=int64)"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"-hdLlwFTzopj","colab_type":"text"},"source":["There were 100 trees for second and third models, but with different criterion. However, both \"gini\" and \"entropy\" criterions has the same result. "]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"TwFTc4jCzopk","colab_type":"text"},"source":["Let's get the proportion of correctly predicted values for frauds only."]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"Z9GVm0Hmzopl","colab_type":"code","colab":{},"outputId":"93a58c6e-09b6-44dd-f8e8-6bba2a4a6051"},"source":["eval_3 = 125/162\n","eval_3"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7716049382716049"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"deletable":true,"editable":true,"id":"7eIOPjR-zopp","colab_type":"code","colab":{},"outputId":"e308fbac-fc68-4461-8ecd-2343599e4a0b"},"source":["eval_1"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7469135802469136"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"Mva4yubNzops","colab_type":"text"},"source":["We were able to improve the prediction score of the Fraudulent transactions by 3% after tuning the parameters"]}]}