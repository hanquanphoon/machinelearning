{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-9j33km-e_B_"
   },
   "source": [
    "# K Nearest Neighbor Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "txIFraeoe_CA"
   },
   "source": [
    "https://stackabuse.com/k-nearest-neighbors-algorithm-in-python-and-scikit-learn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MjUzXv_Re_CB"
   },
   "source": [
    "The K-nearest neighbors (KNN) algorithm is a type of supervised machine learning algorithms. KNN is extremely easy to implement in its most basic form, and yet performs quite complex classification tasks. It is a lazy learning algorithm since it doesn't have a specialized training phase. Rather, it uses all of the data for training while classifying a new data point or instance. KNN is a non-parametric learning algorithm, which means that it doesn't assume anything about the underlying data. This is an extremely useful feature since most of the real world data doesn't really follow any theoretical assumption e.g. linear-separability, uniform distribution, etc.\n",
    "\n",
    "In this article, we will see how KNN can be implemented with Python's Scikit-Learn library. But before that let's first explore the theory behind KNN and see what are some of the pros and cons of the algorithm.\n",
    "\n",
    "# Theory\n",
    "The intuition behind the KNN algorithm is one of the simplest of all the supervised machine learning algorithms. It simply calculates the distance of a new data point to all other training data points. The distance can be of any type e.g Euclidean or Manhattan etc. It then selects the K-nearest data points, where K can be any integer. Finally it assigns the data point to the class to which the majority of the K data points belong.\n",
    "\n",
    "Let's see this algorithm in action with the help of a simple example. Suppose you have a dataset with two variables, which when plotted, looks like the one in the following figure.\n",
    "<img src=\"knn1.jpg\">\n",
    " \n",
    "Your task is to classify a new data point with 'X' into \"Blue\" class or \"Red\" class. The coordinate values of the data point are x=45 and y=50. Suppose the value of K is 3. The KNN algorithm starts by calculating the distance of point X from all the points. It then finds the 3 nearest points with least distance to point X. This is shown in the figure below. The three nearest points have been encircled.\n",
    "<img src=\"knn2.jpg\">\n",
    "\n",
    "                                              Datapoint plot circled\n",
    "The final step of the KNN algorithm is to assign new point to the class to which majority of the three nearest points belong. From the figure above we can see that the two of the three nearest points belong to the class \"Red\" while one belongs to the class \"Blue\". Therefore the new data point will be classified as \"Red\".\n",
    "\n",
    "## Pros and Cons of KNN\n",
    "In this section we'll present some of the pros and cons of using the KNN algorithm.\n",
    "\n",
    "## Pros\n",
    "It is extremely easy to implement\n",
    "As said earlier, it is lazy learning algorithm and therefore requires no training prior to making real time predictions. This makes the KNN algorithm much faster than other algorithms that require training e.g SVM, linear regression, etc.\n",
    "Since the algorithm requires no training before making predictions, new data can be added seamlessly.\n",
    "There are only two parameters required to implement KNN i.e. the value of K and the distance function (e.g. Euclidean or Manhattan etc.)\n",
    "\n",
    "## Cons\n",
    "The KNN algorithm doesn't work well with high dimensional data because with large number of dimensions, it becomes difficult for the algorithm to calculate distance in each dimension.\n",
    "The KNN algorithm has a high prediction cost for large datasets. This is because in large datasets the cost of calculating distance between new point and each existing point becomes higher.\n",
    "Finally, the KNN algorithm doesn't work well with categorical features since it is difficult to find the distance between dimensions with categorical features.\n",
    "\n",
    "## Implementing KNN Algorithm with Scikit-Learn\n",
    "In this section, we will see how Python's Scikit-Learn library can be used to implement the KNN algorithm in less than 20 lines of code. The download and installation instructions for Scikit learn library are available at here.\n",
    "\n",
    "Note: The code provided in this tutorial has been executed and tested with Python Jupyter notebook.\n",
    "\n",
    "## The Dataset\n",
    "We are going to use the famous iris data set for our KNN example. The dataset consists of four attributes: sepal-width, sepal-length, petal-width and petal-length. These are the attributes of specific types of iris plant. The task is to predict the class to which these plants belong. There are three classes in the dataset: Iris-setosa, Iris-versicolor and Iris-virginica. Further details of the dataset are available here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BjuurQhqe_CC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "bBv-Jdmpe_CH"
   },
   "source": [
    "Importing the Dataset\n",
    "To import the dataset and load it into our pandas dataframe, execute the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FKMReYSRe_CI",
    "outputId": "574241f1-9a44-487b-acc2-a2272c746dd5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal-length</th>\n",
       "      <th>sepal-width</th>\n",
       "      <th>petal-length</th>\n",
       "      <th>petal-width</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal-length  sepal-width  petal-length  petal-width           Class\n",
       "0             5.1          3.5           1.4          0.2     Iris-setosa\n",
       "1             4.9          3.0           1.4          0.2     Iris-setosa\n",
       "2             4.7          3.2           1.3          0.2     Iris-setosa\n",
       "3             4.6          3.1           1.5          0.2     Iris-setosa\n",
       "4             5.0          3.6           1.4          0.2     Iris-setosa\n",
       "5             5.4          3.9           1.7          0.4     Iris-setosa\n",
       "6             4.6          3.4           1.4          0.3     Iris-setosa\n",
       "7             5.0          3.4           1.5          0.2     Iris-setosa\n",
       "8             4.4          2.9           1.4          0.2     Iris-setosa\n",
       "9             4.9          3.1           1.5          0.1     Iris-setosa\n",
       "10            5.4          3.7           1.5          0.2     Iris-setosa\n",
       "11            4.8          3.4           1.6          0.2     Iris-setosa\n",
       "12            4.8          3.0           1.4          0.1     Iris-setosa\n",
       "13            4.3          3.0           1.1          0.1     Iris-setosa\n",
       "14            5.8          4.0           1.2          0.2     Iris-setosa\n",
       "15            5.7          4.4           1.5          0.4     Iris-setosa\n",
       "16            5.4          3.9           1.3          0.4     Iris-setosa\n",
       "17            5.1          3.5           1.4          0.3     Iris-setosa\n",
       "18            5.7          3.8           1.7          0.3     Iris-setosa\n",
       "19            5.1          3.8           1.5          0.3     Iris-setosa\n",
       "20            5.4          3.4           1.7          0.2     Iris-setosa\n",
       "21            5.1          3.7           1.5          0.4     Iris-setosa\n",
       "22            4.6          3.6           1.0          0.2     Iris-setosa\n",
       "23            5.1          3.3           1.7          0.5     Iris-setosa\n",
       "24            4.8          3.4           1.9          0.2     Iris-setosa\n",
       "25            5.0          3.0           1.6          0.2     Iris-setosa\n",
       "26            5.0          3.4           1.6          0.4     Iris-setosa\n",
       "27            5.2          3.5           1.5          0.2     Iris-setosa\n",
       "28            5.2          3.4           1.4          0.2     Iris-setosa\n",
       "29            4.7          3.2           1.6          0.2     Iris-setosa\n",
       "..            ...          ...           ...          ...             ...\n",
       "120           6.9          3.2           5.7          2.3  Iris-virginica\n",
       "121           5.6          2.8           4.9          2.0  Iris-virginica\n",
       "122           7.7          2.8           6.7          2.0  Iris-virginica\n",
       "123           6.3          2.7           4.9          1.8  Iris-virginica\n",
       "124           6.7          3.3           5.7          2.1  Iris-virginica\n",
       "125           7.2          3.2           6.0          1.8  Iris-virginica\n",
       "126           6.2          2.8           4.8          1.8  Iris-virginica\n",
       "127           6.1          3.0           4.9          1.8  Iris-virginica\n",
       "128           6.4          2.8           5.6          2.1  Iris-virginica\n",
       "129           7.2          3.0           5.8          1.6  Iris-virginica\n",
       "130           7.4          2.8           6.1          1.9  Iris-virginica\n",
       "131           7.9          3.8           6.4          2.0  Iris-virginica\n",
       "132           6.4          2.8           5.6          2.2  Iris-virginica\n",
       "133           6.3          2.8           5.1          1.5  Iris-virginica\n",
       "134           6.1          2.6           5.6          1.4  Iris-virginica\n",
       "135           7.7          3.0           6.1          2.3  Iris-virginica\n",
       "136           6.3          3.4           5.6          2.4  Iris-virginica\n",
       "137           6.4          3.1           5.5          1.8  Iris-virginica\n",
       "138           6.0          3.0           4.8          1.8  Iris-virginica\n",
       "139           6.9          3.1           5.4          2.1  Iris-virginica\n",
       "140           6.7          3.1           5.6          2.4  Iris-virginica\n",
       "141           6.9          3.1           5.1          2.3  Iris-virginica\n",
       "142           5.8          2.7           5.1          1.9  Iris-virginica\n",
       "143           6.8          3.2           5.9          2.3  Iris-virginica\n",
       "144           6.7          3.3           5.7          2.5  Iris-virginica\n",
       "145           6.7          3.0           5.2          2.3  Iris-virginica\n",
       "146           6.3          2.5           5.0          1.9  Iris-virginica\n",
       "147           6.5          3.0           5.2          2.0  Iris-virginica\n",
       "148           6.2          3.4           5.4          2.3  Iris-virginica\n",
       "149           5.9          3.0           5.1          1.8  Iris-virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "\n",
    "# Assign colum names to the dataset\n",
    "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class']\n",
    "\n",
    "# Read dataset to pandas dataframe\n",
    "dataset = pd.read_csv(url, names=names)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0N7EpwbLe_CN"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wUutGOe2e_CO"
   },
   "source": [
    "The X variable contains the first four columns of the dataset (i.e. attributes) while y contains the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kQp_KtGQe_CP",
    "outputId": "10c898a1-cb39-46ed-b7be-c33234205ebb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Class\n",
       "0       Iris-setosa\n",
       "1       Iris-setosa\n",
       "2       Iris-setosa\n",
       "3       Iris-setosa\n",
       "4       Iris-setosa\n",
       "5       Iris-setosa\n",
       "6       Iris-setosa\n",
       "7       Iris-setosa\n",
       "8       Iris-setosa\n",
       "9       Iris-setosa\n",
       "10      Iris-setosa\n",
       "11      Iris-setosa\n",
       "12      Iris-setosa\n",
       "13      Iris-setosa\n",
       "14      Iris-setosa\n",
       "15      Iris-setosa\n",
       "16      Iris-setosa\n",
       "17      Iris-setosa\n",
       "18      Iris-setosa\n",
       "19      Iris-setosa\n",
       "20      Iris-setosa\n",
       "21      Iris-setosa\n",
       "22      Iris-setosa\n",
       "23      Iris-setosa\n",
       "24      Iris-setosa\n",
       "25      Iris-setosa\n",
       "26      Iris-setosa\n",
       "27      Iris-setosa\n",
       "28      Iris-setosa\n",
       "29      Iris-setosa\n",
       "..              ...\n",
       "120  Iris-virginica\n",
       "121  Iris-virginica\n",
       "122  Iris-virginica\n",
       "123  Iris-virginica\n",
       "124  Iris-virginica\n",
       "125  Iris-virginica\n",
       "126  Iris-virginica\n",
       "127  Iris-virginica\n",
       "128  Iris-virginica\n",
       "129  Iris-virginica\n",
       "130  Iris-virginica\n",
       "131  Iris-virginica\n",
       "132  Iris-virginica\n",
       "133  Iris-virginica\n",
       "134  Iris-virginica\n",
       "135  Iris-virginica\n",
       "136  Iris-virginica\n",
       "137  Iris-virginica\n",
       "138  Iris-virginica\n",
       "139  Iris-virginica\n",
       "140  Iris-virginica\n",
       "141  Iris-virginica\n",
       "142  Iris-virginica\n",
       "143  Iris-virginica\n",
       "144  Iris-virginica\n",
       "145  Iris-virginica\n",
       "146  Iris-virginica\n",
       "147  Iris-virginica\n",
       "148  Iris-virginica\n",
       "149  Iris-virginica\n",
       "\n",
       "[150 rows x 1 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=dataset.iloc[:,:4]\n",
    "y=dataset.iloc[:,-1:]\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WKWb6-Ode_CS"
   },
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XcVP51x2e_CT",
    "outputId": "2595f96d-e9d1-4abc-8023-8e9e71873255"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal-length</th>\n",
       "      <th>sepal-width</th>\n",
       "      <th>petal-length</th>\n",
       "      <th>petal-width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>5.2</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>6.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>4.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>7.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>6.6</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>5.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>5.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.9</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal-length  sepal-width  petal-length  petal-width\n",
       "137           6.4          3.1           5.5          1.8\n",
       "84            5.4          3.0           4.5          1.5\n",
       "27            5.2          3.5           1.5          0.2\n",
       "127           6.1          3.0           4.9          1.8\n",
       "132           6.4          2.8           5.6          2.2\n",
       "59            5.2          2.7           3.9          1.4\n",
       "18            5.7          3.8           1.7          0.3\n",
       "83            6.0          2.7           5.1          1.6\n",
       "61            5.9          3.0           4.2          1.5\n",
       "92            5.8          2.6           4.0          1.2\n",
       "112           6.8          3.0           5.5          2.1\n",
       "2             4.7          3.2           1.3          0.2\n",
       "141           6.9          3.1           5.1          2.3\n",
       "43            5.0          3.5           1.6          0.6\n",
       "10            5.4          3.7           1.5          0.2\n",
       "60            5.0          2.0           3.5          1.0\n",
       "116           6.5          3.0           5.5          1.8\n",
       "144           6.7          3.3           5.7          2.5\n",
       "119           6.0          2.2           5.0          1.5\n",
       "108           6.7          2.5           5.8          1.8\n",
       "69            5.6          2.5           3.9          1.1\n",
       "135           7.7          3.0           6.1          2.3\n",
       "56            6.3          3.3           4.7          1.6\n",
       "80            5.5          2.4           3.8          1.1\n",
       "123           6.3          2.7           4.9          1.8\n",
       "133           6.3          2.8           5.1          1.5\n",
       "106           4.9          2.5           4.5          1.7\n",
       "146           6.3          2.5           5.0          1.9\n",
       "50            7.0          3.2           4.7          1.4\n",
       "147           6.5          3.0           5.2          2.0\n",
       "..            ...          ...           ...          ...\n",
       "14            5.8          4.0           1.2          0.2\n",
       "122           7.7          2.8           6.7          2.0\n",
       "19            5.1          3.8           1.5          0.3\n",
       "29            4.7          3.2           1.6          0.2\n",
       "130           7.4          2.8           6.1          1.9\n",
       "49            5.0          3.3           1.4          0.2\n",
       "136           6.3          3.4           5.6          2.4\n",
       "99            5.7          2.8           4.1          1.3\n",
       "82            5.8          2.7           3.9          1.2\n",
       "79            5.7          2.6           3.5          1.0\n",
       "115           6.4          3.2           5.3          2.3\n",
       "145           6.7          3.0           5.2          2.3\n",
       "72            6.3          2.5           4.9          1.5\n",
       "77            6.7          3.0           5.0          1.7\n",
       "25            5.0          3.0           1.6          0.2\n",
       "81            5.5          2.4           3.7          1.0\n",
       "140           6.7          3.1           5.6          2.4\n",
       "142           5.8          2.7           5.1          1.9\n",
       "39            5.1          3.4           1.5          0.2\n",
       "58            6.6          2.9           4.6          1.3\n",
       "88            5.6          3.0           4.1          1.3\n",
       "70            5.9          3.2           4.8          1.8\n",
       "87            6.3          2.3           4.4          1.3\n",
       "36            5.5          3.5           1.3          0.2\n",
       "21            5.1          3.7           1.5          0.4\n",
       "9             4.9          3.1           1.5          0.1\n",
       "103           6.3          2.9           5.6          1.8\n",
       "67            5.8          2.7           4.1          1.0\n",
       "117           7.7          3.8           6.7          2.2\n",
       "47            4.6          3.2           1.4          0.2\n",
       "\n",
       "[120 rows x 4 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20,random_state=0)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vh3ug0iNe_CW"
   },
   "source": [
    "# Feature Scaling\n",
    "\n",
    "Before making any actual predictions, it is always a good practice to scale the features so that all of them can be uniformly evaluated. Wikipedia explains the reasoning pretty well:\n",
    "\n",
    "Since the range of values of raw data varies widely, in some machine learning algorithms, objective functions will not work properly without normalization. For example, the majority of classifiers calculate the distance between two points by the Euclidean distance. If one of the features has a broad range of values, the distance will be governed by this particular feature. Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance.\n",
    "\n",
    "The gradient descent algorithm (which is used in neural network training and other machine learning algorithms) also converges faster with normalized features.\n",
    "\n",
    "The following script performs feature scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "chFLdY4be_CX",
    "outputId": "21b63fff-7eba-40e6-d80a-1752b4aaff21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.61303014,  0.10850105,  0.94751783,  0.73603967],\n",
       "       [-0.56776627, -0.12400121,  0.38491447,  0.34808318],\n",
       "       [-0.80392556,  1.03851009, -1.30289562, -1.3330616 ],\n",
       "       [ 0.25879121, -0.12400121,  0.60995581,  0.73603967],\n",
       "       [ 0.61303014, -0.58900572,  1.00377816,  1.25331499],\n",
       "       [-0.80392556, -0.82150798,  0.04735245,  0.21876435],\n",
       "       [-0.21352735,  1.73601687, -1.19037495, -1.20374277],\n",
       "       [ 0.14071157, -0.82150798,  0.72247648,  0.47740201],\n",
       "       [ 0.02263193, -0.12400121,  0.21613346,  0.34808318],\n",
       "       [-0.09544771, -1.05401024,  0.10361279, -0.03987331],\n",
       "       [ 1.0853487 , -0.12400121,  0.94751783,  1.12399616],\n",
       "       [-1.39432376,  0.34100331, -1.41541629, -1.3330616 ],\n",
       "       [ 1.20342834,  0.10850105,  0.72247648,  1.38263382],\n",
       "       [-1.04008484,  1.03851009, -1.24663528, -0.81578628],\n",
       "       [-0.56776627,  1.50351461, -1.30289562, -1.3330616 ],\n",
       "       [-1.04008484, -2.4490238 , -0.1776889 , -0.29851096],\n",
       "       [ 0.73110978, -0.12400121,  0.94751783,  0.73603967],\n",
       "       [ 0.96726906,  0.57350557,  1.0600385 ,  1.64127148],\n",
       "       [ 0.14071157, -1.98401928,  0.66621615,  0.34808318],\n",
       "       [ 0.96726906, -1.2865125 ,  1.11629884,  0.73603967],\n",
       "       [-0.33160699, -1.2865125 ,  0.04735245, -0.16919214],\n",
       "       [ 2.14806547, -0.12400121,  1.28507985,  1.38263382],\n",
       "       [ 0.49495049,  0.57350557,  0.49743514,  0.47740201],\n",
       "       [-0.44968663, -1.51901476, -0.00890789, -0.16919214],\n",
       "       [ 0.49495049, -0.82150798,  0.60995581,  0.73603967],\n",
       "       [ 0.49495049, -0.58900572,  0.72247648,  0.34808318],\n",
       "       [-1.15816448, -1.2865125 ,  0.38491447,  0.60672084],\n",
       "       [ 0.49495049, -1.2865125 ,  0.66621615,  0.8653585 ],\n",
       "       [ 1.32150798,  0.34100331,  0.49743514,  0.21876435],\n",
       "       [ 0.73110978, -0.12400121,  0.77873682,  0.99467733],\n",
       "       [ 0.14071157,  0.80600783,  0.38491447,  0.47740201],\n",
       "       [-1.27624412,  0.10850105, -1.24663528, -1.3330616 ],\n",
       "       [-0.09544771, -0.82150798,  0.72247648,  0.8653585 ],\n",
       "       [-0.33160699, -0.82150798,  0.21613346,  0.08944552],\n",
       "       [-0.33160699, -0.35650346, -0.12142856,  0.08944552],\n",
       "       [-0.44968663, -1.2865125 ,  0.10361279,  0.08944552],\n",
       "       [ 0.25879121, -0.12400121,  0.4411748 ,  0.21876435],\n",
       "       [ 1.55766726,  0.34100331,  1.22881951,  0.73603967],\n",
       "       [-0.68584591,  1.50351461, -1.30289562, -1.3330616 ],\n",
       "       [-1.86664232, -0.12400121, -1.52793696, -1.46238043],\n",
       "       [ 0.61303014, -0.82150798,  0.83499716,  0.8653585 ],\n",
       "       [-0.21352735, -0.12400121,  0.21613346, -0.03987331],\n",
       "       [-0.56776627,  0.80600783, -1.19037495, -1.3330616 ],\n",
       "       [-0.21352735,  3.13103043, -1.30289562, -1.07442394],\n",
       "       [ 1.20342834,  0.10850105,  0.60995581,  0.34808318],\n",
       "       [-1.5124034 ,  0.10850105, -1.30289562, -1.3330616 ],\n",
       "       [ 0.02263193, -0.12400121,  0.72247648,  0.73603967],\n",
       "       [-0.9220052 , -1.2865125 , -0.45899058, -0.16919214],\n",
       "       [-1.5124034 ,  0.80600783, -1.35915595, -1.20374277],\n",
       "       [ 0.37687085, -1.98401928,  0.38491447,  0.34808318],\n",
       "       [ 1.55766726,  1.27101235,  1.28507985,  1.64127148],\n",
       "       [-0.21352735, -0.35650346,  0.21613346,  0.08944552],\n",
       "       [-1.27624412, -0.12400121, -1.35915595, -1.46238043],\n",
       "       [ 1.43958762, -0.12400121,  1.17255917,  1.12399616],\n",
       "       [ 1.20342834,  0.34100331,  1.0600385 ,  1.38263382],\n",
       "       [ 0.73110978, -0.12400121,  1.11629884,  1.25331499],\n",
       "       [ 0.61303014, -0.58900572,  1.00377816,  1.12399616],\n",
       "       [-0.9220052 ,  1.73601687, -1.24663528, -1.3330616 ],\n",
       "       [-1.27624412,  0.80600783, -1.24663528, -1.3330616 ],\n",
       "       [ 0.73110978,  0.34100331,  0.72247648,  0.99467733],\n",
       "       [ 0.96726906,  0.57350557,  1.0600385 ,  1.12399616],\n",
       "       [-1.63048304, -1.75151702, -1.41541629, -1.20374277],\n",
       "       [ 0.37687085,  0.80600783,  0.89125749,  1.38263382],\n",
       "       [-1.15816448, -0.12400121, -1.35915595, -1.3330616 ],\n",
       "       [-0.21352735, -1.2865125 ,  0.66621615,  0.99467733],\n",
       "       [ 1.20342834,  0.10850105,  0.89125749,  1.12399616],\n",
       "       [-1.74856268,  0.34100331, -1.41541629, -1.3330616 ],\n",
       "       [-1.04008484,  1.27101235, -1.35915595, -1.3330616 ],\n",
       "       [ 1.55766726, -0.12400121,  1.11629884,  0.47740201],\n",
       "       [-0.9220052 ,  1.03851009, -1.35915595, -1.20374277],\n",
       "       [-1.74856268, -0.12400121, -1.41541629, -1.3330616 ],\n",
       "       [-0.56776627,  1.96851913, -1.19037495, -1.07442394],\n",
       "       [-0.44968663, -1.75151702,  0.10361279,  0.08944552],\n",
       "       [ 1.0853487 ,  0.34100331,  1.17255917,  1.38263382],\n",
       "       [ 2.02998583, -0.12400121,  1.56638153,  1.12399616],\n",
       "       [-0.9220052 ,  1.03851009, -1.35915595, -1.3330616 ],\n",
       "       [-1.15816448,  0.10850105, -1.30289562, -1.46238043],\n",
       "       [-0.80392556,  0.80600783, -1.35915595, -1.3330616 ],\n",
       "       [-0.21352735, -0.58900572,  0.38491447,  0.08944552],\n",
       "       [ 0.84918942, -0.12400121,  0.32865413,  0.21876435],\n",
       "       [-1.04008484,  0.34100331, -1.47167663, -1.3330616 ],\n",
       "       [-0.9220052 ,  0.57350557, -1.19037495, -0.94510511],\n",
       "       [ 0.61303014, -0.35650346,  0.27239379,  0.08944552],\n",
       "       [-0.56776627,  0.80600783, -1.30289562, -1.07442394],\n",
       "       [ 2.14806547, -1.05401024,  1.73516253,  1.38263382],\n",
       "       [-1.15816448, -1.51901476, -0.29020957, -0.29851096],\n",
       "       [ 2.38422475,  1.73601687,  1.45386085,  0.99467733],\n",
       "       [ 0.96726906,  0.10850105,  0.32865413,  0.21876435],\n",
       "       [-0.80392556,  2.43352365, -1.30289562, -1.46238043],\n",
       "       [ 0.14071157, -0.12400121,  0.55369548,  0.73603967],\n",
       "       [-0.09544771,  2.20102139, -1.47167663, -1.3330616 ],\n",
       "       [ 2.14806547, -0.58900572,  1.62264186,  0.99467733],\n",
       "       [-0.9220052 ,  1.73601687, -1.30289562, -1.20374277],\n",
       "       [-1.39432376,  0.34100331, -1.24663528, -1.3330616 ],\n",
       "       [ 1.79382654, -0.58900572,  1.28507985,  0.8653585 ],\n",
       "       [-1.04008484,  0.57350557, -1.35915595, -1.3330616 ],\n",
       "       [ 0.49495049,  0.80600783,  1.00377816,  1.51195265],\n",
       "       [-0.21352735, -0.58900572,  0.15987312,  0.08944552],\n",
       "       [-0.09544771, -0.82150798,  0.04735245, -0.03987331],\n",
       "       [-0.21352735, -1.05401024, -0.1776889 , -0.29851096],\n",
       "       [ 0.61303014,  0.34100331,  0.83499716,  1.38263382],\n",
       "       [ 0.96726906, -0.12400121,  0.77873682,  1.38263382],\n",
       "       [ 0.49495049, -1.2865125 ,  0.60995581,  0.34808318],\n",
       "       [ 0.96726906, -0.12400121,  0.66621615,  0.60672084],\n",
       "       [-1.04008484, -0.12400121, -1.24663528, -1.3330616 ],\n",
       "       [-0.44968663, -1.51901476, -0.06516822, -0.29851096],\n",
       "       [ 0.96726906,  0.10850105,  1.00377816,  1.51195265],\n",
       "       [-0.09544771, -0.82150798,  0.72247648,  0.8653585 ],\n",
       "       [-0.9220052 ,  0.80600783, -1.30289562, -1.3330616 ],\n",
       "       [ 0.84918942, -0.35650346,  0.4411748 ,  0.08944552],\n",
       "       [-0.33160699, -0.12400121,  0.15987312,  0.08944552],\n",
       "       [ 0.02263193,  0.34100331,  0.55369548,  0.73603967],\n",
       "       [ 0.49495049, -1.75151702,  0.32865413,  0.08944552],\n",
       "       [-0.44968663,  1.03851009, -1.41541629, -1.3330616 ],\n",
       "       [-0.9220052 ,  1.50351461, -1.30289562, -1.07442394],\n",
       "       [-1.15816448,  0.10850105, -1.30289562, -1.46238043],\n",
       "       [ 0.49495049, -0.35650346,  1.00377816,  0.73603967],\n",
       "       [-0.09544771, -0.82150798,  0.15987312, -0.29851096],\n",
       "       [ 2.14806547,  1.73601687,  1.62264186,  1.25331499],\n",
       "       [-1.5124034 ,  0.34100331, -1.35915595, -1.3330616 ]])"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x_scale= scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "TxKo6UtDe_Cb"
   },
   "source": [
    "# Training and Predictions\n",
    "It is extremely straight forward to train the KNN algorithm and make predictions with it, especially when using Scikit-Learn.The first step is to import the KNeighborsClassifier class from the sklearn.neighbors library. In the second line, this class is initialized with one parameter, i.e. n_neigbours. This is basically the value for the K. There is no ideal value for K and it is selected after testing and evaluation, however to start out, 5 seems to be the most commonly used value for KNN algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V1xrPayee_Cc",
    "outputId": "e43c2fc8-5e94-4cb0-b3f1-82395084f19d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\ipykernel\\__main__.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vCkiERh9e_Cg",
    "outputId": "6ffb8d1c-cb55-443f-c7f4-6e42036d7673"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Iris-virginica', 'Iris-versicolor', 'Iris-setosa',\n",
       "       'Iris-virginica', 'Iris-setosa', 'Iris-virginica', 'Iris-setosa',\n",
       "       'Iris-versicolor', 'Iris-versicolor', 'Iris-versicolor',\n",
       "       'Iris-virginica', 'Iris-versicolor', 'Iris-versicolor',\n",
       "       'Iris-versicolor', 'Iris-versicolor', 'Iris-setosa',\n",
       "       'Iris-versicolor', 'Iris-versicolor', 'Iris-setosa', 'Iris-setosa',\n",
       "       'Iris-virginica', 'Iris-versicolor', 'Iris-setosa', 'Iris-setosa',\n",
       "       'Iris-virginica', 'Iris-setosa', 'Iris-setosa', 'Iris-versicolor',\n",
       "       'Iris-versicolor', 'Iris-setosa'], dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5b4HUf1e_Ck"
   },
   "source": [
    "# Evaluating the Algorithm\n",
    "<pr>For evaluating an algorithm, confusion matrix, precision, recall and f1 score are the most commonly used metrics. The <b>confusion_matrix</b> and <b>classification_report</b> methods of the <b>sklearn.metrics</b> can be used to calculate these metrics. Take a look at the following script:</pr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pbA75NFDe_Cl",
    "outputId": "8f8f67ec-9abf-4210-9abb-2ed9e42a1328"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0  6]]\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        11\n",
      "Iris-versicolor       1.00      1.00      1.00        13\n",
      " Iris-virginica       1.00      1.00      1.00         6\n",
      "\n",
      "    avg / total       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IJIV7163e_Cp"
   },
   "source": [
    "# Comparing Error Rate with the K Value\n",
    "In the training and prediction section we said that there is no way to know beforehand which value of K that yields the best results in the first go. We randomly chose 5 as the K value and it just happen to result in 100% accuracy.\n",
    "\n",
    "One way to help you find the best value of K is to plot the graph of K value and the corresponding error rate for the dataset.\n",
    "\n",
    "In this section, we will plot the mean error for the predicted values of test set for all the K values between 1 and 40.\n",
    "\n",
    "To do so, let's first calculate the mean of error for all the predicted values where K ranges from 1 and 40. Execute the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DTNfWlJ7e_Cq",
    "outputId": "91c09566-62bc-431f-c368-3ceba71e8780"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Class    0.033333\n",
      "dtype: float64, Class    0.033333\n",
      "dtype: float64, Class    0.033333\n",
      "dtype: float64, Class    0.033333\n",
      "dtype: float64, Class    0.0\n",
      "dtype: float64, Class    0.0\n",
      "dtype: float64, Class    0.0\n",
      "dtype: float64, Class    0.0\n",
      "dtype: float64, Class    0.0\n",
      "dtype: float64, Class    0.0\n",
      "dtype: float64, Class    0.0\n",
      "dtype: float64, Class    0.0\n",
      "dtype: float64, Class    0.0\n",
      "dtype: float64, Class    0.0\n",
      "dtype: float64, Class    0.0\n",
      "dtype: float64, Class    0.0\n",
      "dtype: float64, Class    0.0\n",
      "dtype: float64, Class    0.0\n",
      "dtype: float64, Class    0.033333\n",
      "dtype: float64, Class    0.033333\n",
      "dtype: float64, Class    0.1\n",
      "dtype: float64, Class    0.133333\n",
      "dtype: float64, Class    0.133333\n",
      "dtype: float64, Class    0.1\n",
      "dtype: float64, Class    0.1\n",
      "dtype: float64, Class    0.1\n",
      "dtype: float64, Class    0.1\n",
      "dtype: float64, Class    0.1\n",
      "dtype: float64, Class    0.1\n",
      "dtype: float64]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Enthought\\Canopy\\edm\\envs\\User\\lib\\site-packages\\ipykernel\\__main__.py:6: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "error = []\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Calculating error for K values between 1 and 30\n",
    "for i in range(1,30):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train, y_train)\n",
    "    pred_i = knn.predict(X_test).tolist()\n",
    "    error.append( np.mean(pred_i!=y_test ))\n",
    "print(error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TXLqBY9se_Ct",
    "outputId": "9f6e0de4-bfa6-4e4e-f19a-a2f674fb857e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Class\n",
      "82   False\n",
      "132  False\n",
      "48   False\n",
      "144  False\n",
      "91   False\n",
      "35   False\n",
      "57   False\n",
      "124  False\n",
      "30   False\n",
      "80   False\n",
      "13   False\n",
      "112  False\n",
      "56   False\n",
      "3    False\n",
      "130  False\n",
      "12   False\n",
      "100  False\n",
      "69   False\n",
      "117  False\n",
      "101  False\n",
      "52    True\n",
      "98   False\n",
      "125  False\n",
      "92   False\n",
      "129  False\n",
      "60   False\n",
      "65   False\n",
      "8    False\n",
      "105  False\n",
      "45   False\n"
     ]
    }
   ],
   "source": [
    "print((pred_i!=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WSPTN14qe_Cw"
   },
   "source": [
    "The above script executes a loop from 1 to 40. In each iteration the mean error for predicted values of test set is calculated and the result is appended to the error list.\n",
    "\n",
    "The next step is to plot the error values against K values. Execute the following script to create the plot:this plot shows how error will change if you increase the number of neighbores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ryh0ZjE4e_Cx",
    "outputId": "d002829c-e90b-445b-8a10-e2155fde685a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtoAAAGDCAYAAAAVh7eRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb8d1216b00>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, 30), error, color='red', linestyle='dashed', marker='o',markerfacecolor='blue', markersize=10)\n",
    "plt.title('Error Rate K Value')\n",
    "plt.xlabel('K Value')\n",
    "plt.ylabel('Mean Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2-AaPxTEe_C0"
   },
   "source": [
    "# Conclusion\n",
    "KNN is a simple yet powerful classification algorithm. It requires no training for making predictions, which is typically one of the most difficult parts of a machine learning algorithm. The KNN algorithm have been widely used to find document similarity and pattern recognition. It has also been employed for developing recommender systems and for dimensionality reduction and pre-processing steps for computer vision, particularly face recognition tasks.\n",
    "\n",
    "From here, I would advise you to implement the KNN algorithm for a different classification dataset. Vary the test and training size along with the K value to see how your results differ and how can you improve the accuracy of your algorithm. A good collection of classification datasets is available here for you to play with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T6wkwVWhe_C1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "1-KNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
